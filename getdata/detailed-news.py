from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.common.exceptions import NoSuchElementException
import mysql.connector
import time
import re
import unicodedata

# H√†m t·∫°o slug t·ª´ ti√™u ƒë·ªÅ
def create_slug(title, connection=None):
    if not title:
        return ""
    
    # Chuy·ªÉn ƒë·ªïi unicode th√†nh ASCII
    slug = unicodedata.normalize('NFKD', title).encode('ascii', 'ignore').decode('ascii')
    # Chuy·ªÉn th√†nh ch·ªØ th∆∞·ªùng
    slug = slug.lower()
    # Thay th·∫ø c√°c k√Ω t·ª± kh√¥ng ph·∫£i ch·ªØ c√°i, s·ªë ho·∫∑c d·∫•u g·∫°ch ngang b·∫±ng d·∫•u g·∫°ch ngang
    slug = re.sub(r'[^a-z0-9]+', '-', slug)
    # Lo·∫°i b·ªè d·∫•u g·∫°ch ngang ·ªü ƒë·∫ßu v√† cu·ªëi
    slug = slug.strip('-')
    # Gi·ªõi h·∫°n ƒë·ªô d√†i slug
    slug = slug[:100]
    
    # Ki·ªÉm tra slug c√≥ t·ªìn t·∫°i trong DB kh√¥ng (n·∫øu c√≥ connection)
    if connection and connection.is_connected():
        try:
            cursor = connection.cursor()
            cursor.execute("SELECT COUNT(*) FROM detailed_news WHERE slug = %s", (slug,))
            count = cursor.fetchone()[0]
            cursor.close()
            
            if count > 0:
                # Th√™m timestamp n·∫øu slug ƒë√£ t·ªìn t·∫°i
                slug = f"{slug}-{int(time.time())}"
        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói khi ki·ªÉm tra slug: {str(e)}")
    
    return slug

# Kh·ªüi t·∫°o tr√¨nh duy·ªát
driver = webdriver.Chrome()
specific_url = "https://edition.cnn.com/2025/04/03/science/ancient-dna-green-sahara-mummies/index.html"
driver.get(specific_url)
print(f"ƒê√£ truy c·∫≠p trang: {specific_url}")
time.sleep(5)

connection = None

try:
    # K·∫øt n·ªëi MySQL
    connection = mysql.connector.connect(
        host='127.0.0.1',
        database='bao',
        user='root',
        password=''
    )
    print('‚úÖ ƒê√£ k·∫øt n·ªëi MySQL')

    article_data = {
        'url': specific_url,
        'title': '',
        'slug': '',
        'author': '',
        'image_url': '',
        'image_caption': '',
        'content': '',
        'subheader': '',
        'inline_images': [],
        'publication_date': None
    }

    # 1. L·∫•y ti√™u ƒë·ªÅ v√† t·∫°o slug
    try:
        title_element = driver.find_element(By.XPATH, '//h1[@data-editable="headlineText"]')
        article_data['title'] = title_element.text.strip()
    except NoSuchElementException:
        try:
            title_element = driver.find_element(By.XPATH, '//h1[contains(@class,"headline__text")]')
            article_data['title'] = title_element.text.strip()
        except NoSuchElementException:
            print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y ti√™u ƒë·ªÅ (h1)")
    
    # T·∫°o slug sau khi c√≥ title
    if article_data['title']:
        article_data['slug'] = create_slug(article_data['title'], connection)

    # 2. T√°c gi·∫£
    try:
        author_elements = driver.find_elements(By.XPATH, '//span[@class="byline__name"]')
        if author_elements:
            article_data['author'] = ', '.join([elem.text for elem in author_elements if elem.text])
        else:
            try:
                byline_element = driver.find_element(By.XPATH, '//div[@class="byline__names"]')
                if byline_element:
                    text = byline_element.text
                    if text.startswith("By "):
                        article_data['author'] = text[3:].split(',')[0].strip()
            except NoSuchElementException:
                pass
    except NoSuchElementException:
        print("‚ÑπÔ∏è Kh√¥ng t√¨m th·∫•y t√°c gi·∫£")

    # 3. ·∫¢nh ƒë·∫°i di·ªán
    try:
        main_img_element = driver.find_element(By.XPATH, '(//img[contains(@class, "image__dam-img")])[1]')
        article_data['image_url'] = main_img_element.get_attribute("src") or main_img_element.get_attribute("data-src") or ""
    except NoSuchElementException:
        print("‚ÑπÔ∏è Kh√¥ng t√¨m th·∫•y ·∫£nh ƒë·∫°i di·ªán")

    # 4. Ch√∫ th√≠ch ·∫£nh
    try:
        caption_element = driver.find_element(By.XPATH, '//div[contains(@class, "image__caption") and contains(@class, "attribution")]//span[@data-editable="metaCaption"]')
        article_data['image_caption'] = caption_element.text.strip()
    except NoSuchElementException:
        print("‚ÑπÔ∏è Kh√¥ng t√¨m th·∫•y ch√∫ th√≠ch ·∫£nh")

    # 5. N·ªôi dung b√†i vi·∫øt
    try:
        paragraph_elements = driver.find_elements(By.XPATH, '//p[contains(@class, "paragraph") and contains(@class, "inline-placeholder") and contains(@class, "vossi-paragraph")]')
        content_parts = [p.text.strip() for p in paragraph_elements if p.text.strip()]
        article_data['content'] = "\n\n".join(content_parts)
    except NoSuchElementException:
        print("‚ÑπÔ∏è Kh√¥ng t√¨m th·∫•y n·ªôi dung b√†i vi·∫øt")

    # 6. Subheader
    try:
        subheader_element = driver.find_element(By.XPATH, '(//div[contains(@class,"article__content")]//h2)[1]')
        article_data['subheader'] = subheader_element.text.strip()
    except NoSuchElementException:
        pass

    # 7. Ng√†y xu·∫•t b·∫£n (n·∫øu c√≥)
    try:
        date_element = driver.find_element(By.XPATH, '//div[contains(@class, "timestamp")]')
        article_data['publication_date'] = date_element.text.strip()
    except NoSuchElementException:
        print("‚ÑπÔ∏è Kh√¥ng t√¨m th·∫•y ng√†y xu·∫•t b·∫£n")

    # In th√¥ng tin l·∫•y ƒë∆∞·ª£c
    print("\nTh√¥ng tin b√†i vi·∫øt:")
    print(f"  Ti√™u ƒë·ªÅ: {article_data['title']}")
    print(f"  Slug: {article_data['slug']}")
    print(f"  T√°c gi·∫£: {article_data['author']}")
    print(f"  ·∫¢nh: {article_data['image_url'][:70]}..." if article_data['image_url'] else "  ·∫¢nh: Kh√¥ng c√≥")
    print(f"  Caption: {article_data['image_caption']}")
    print(f"  N·ªôi dung: {article_data['content'][:200]}..." if article_data['content'] else "  N·ªôi dung: Kh√¥ng c√≥")
    print(f"  Subheader: {article_data['subheader']}")
    print(f"  Ng√†y xu·∫•t b·∫£n: {article_data['publication_date']}")

    # L∆∞u v√†o database
    if article_data['title'] and article_data['url']:
        cursor = connection.cursor()
        sql = """INSERT INTO detailed_news
                 (url, title, slug, author, image_url, image_caption, content, subheader, publication_date)
                 VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)
                 ON DUPLICATE KEY UPDATE
                 title = VALUES(title),
                 slug = VALUES(slug),
                 author = VALUES(author),
                 image_url = VALUES(image_url),
                 image_caption = VALUES(image_caption),
                 content = VALUES(content),
                 subheader = VALUES(subheader),
                 publication_date = VALUES(publication_date),
                 scraped_at = CURRENT_TIMESTAMP"""

        values = (
            article_data['url'],
            article_data['title'],
            article_data['slug'],
            article_data['author'],
            article_data['image_url'],
            article_data['image_caption'],
            article_data['content'],
            article_data['subheader'],
            article_data['publication_date']
        )
        cursor.execute(sql, values)
        connection.commit()
        cursor.close()
        print(f"\n‚úÖ ƒê√£ l∆∞u/c·∫≠p nh·∫≠t b√†i vi·∫øt v√†o database")
    else:
        print("\n‚ö†Ô∏è Kh√¥ng l∆∞u v√†o database do thi·∫øu ti√™u ƒë·ªÅ ho·∫∑c URL")

except mysql.connector.Error as err:
    print(f"‚ùå L·ªói MySQL: {err}")
except Exception as e:
    print(f"‚ùå L·ªói: {str(e)}")
finally:
    if 'driver' in locals() and driver:
        driver.quit()
        print("‚úÖ ƒê√£ ƒë√≥ng tr√¨nh duy·ªát")
    if connection and connection.is_connected():
        connection.close()
        print("‚úÖ ƒê√£ ƒë√≥ng k·∫øt n·ªëi database")
    print("üõë K·∫øt th√∫c ch∆∞∆°ng tr√¨nh")